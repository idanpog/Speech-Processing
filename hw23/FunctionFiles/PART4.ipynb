{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions and Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from scipy.signal import correlate, find_peaks, lfilter, freqz, resample\n",
    "from scipy.linalg import solve_toeplitz\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from pydub import AudioSegment\n",
    "from math import pi\n",
    "\n",
    "##additional imports for metrics\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.functional.audio.pesq import perceptual_evaluation_speech_quality\n",
    "from torchmetrics.functional import signal_noise_ratio\n",
    "# For displaying interactive plots in the notebook\n",
    "%matplotlib widget\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "def read_wav_file(filename: str) -> (int, np.ndarray):\n",
    "    Fs, signal = read(filename)\n",
    "    signal = signal.astype(np.float32)\n",
    "    signal = signal/(2**15)\n",
    "    return Fs, signal\n",
    "\n",
    "def write_wav_file(filename: str, sampling_rate: int, signal: np.ndarray):\n",
    "    signal = signal*(2**15)\n",
    "    signal = signal.astype(np.int16)    \n",
    "    write(filename, sampling_rate, signal)\n",
    "\n",
    "def lpc(x: np.ndarray, p: int) -> np.ndarray:\n",
    "    n = len(x)\n",
    "    # Compute autocorrelation vector or matrix\n",
    "    nextpow2 = 2**(2*n-1).bit_length()\n",
    "    x_pad = np.pad(array=x, pad_width=(0, nextpow2-n))\n",
    "    X = np.fft.fft(x_pad)\n",
    "    R = np.fft.ifft(np.abs(X)**2)\n",
    "    R = R/n # Biased autocorrelation estimate\n",
    "    a = np.real(solve_toeplitz(R[0:p],-R[1:(p+1)]))\n",
    "    a = np.insert(a,0,1)\n",
    "    return a\n",
    "    \n",
    "def plot_pitch(signal: np.ndarray, pitch_vec: np.ndarray, sampling_frequency: int, frame_length: int):\n",
    "    stvec = create_time_vector(signal.size, sampling_frequency)\n",
    "    ptvec = np.arange(frame_length//2,signal.size,frame_length)/sampling_frequency\n",
    "    pitch_vec_hz = np.zeros(pitch_vec.size)\n",
    "    pitch_vec_hz[pitch_vec != 0] = sampling_frequency/pitch_vec[pitch_vec != 0]\n",
    "    # plot the results\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(stvec,signal)\n",
    "    plt.xlabel('Time [Sec]')\n",
    "    plt.ylabel('Amplitude [Volt]')\n",
    "    plt.title('Speech Signal')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.stem(ptvec,pitch_vec_hz)\n",
    "    plt.xlabel('Time [Sec]')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.title('Pitch Frequency values')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_frame_and_filter_spectrum(fvec: np.ndarray, frameSpec_dB: np.ndarray, filterSpec_dB: np.ndarray, filterOrder: int):\n",
    "    basic_plot(fvec, frameSpec_dB, 'Frequency [Hz]', 'Amplitude [dB]', f'Spectrum of voiced frame and spectrum of predictor filter of order {filterOrder}')\n",
    "    plt.plot(fvec, filterSpec_dB)\n",
    "    y_max = np.max([np.max(frameSpec_dB), np.max(filterSpec_dB)])\n",
    "    y_min = np.min([np.min(frameSpec_dB), np.min(filterSpec_dB)])\n",
    "    y_range = y_max-y_min\n",
    "    _,_,_,_ = plt.axis((fvec[0], fvec[fvec.size-1], y_min-0.1*y_range, y_max+0.1*y_range))         \n",
    "    \n",
    "\n",
    "def create_signals_for_check(signal: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    frame_len = 256\n",
    "    reminder_samp = signal.size%frame_len\n",
    "    if(reminder_samp > 0):\n",
    "        pad_num = (frame_len-reminder_samp) # number of zeros for padding\n",
    "        clean = np.pad(array=signal, pad_width=(0, pad_num))\n",
    "    else:\n",
    "        clean = signal      \n",
    "\n",
    "    # (1) ennode and decode the signal\n",
    "    N,p,lp,e = encoder(signal)\n",
    "    reconstructed = decoder(N, p, lp, e)\n",
    "\n",
    "    # (2) create a broken signal\n",
    "    noisy = clean + np.sqrt(4*10e-6)*np.random.randn(clean.size)\n",
    "    frames_num = noisy.size//frame_len\n",
    "    noisy_frames = np.reshape(a=noisy, newshape=(frames_num, frame_len)) #Each 256 slice is a row   \n",
    "    noisy_frames[np.arange(4,frames_num,8),:] = 0\n",
    "    noisy_frames[np.arange(5,frames_num,8),:] = 0\n",
    "    noisy_frames[np.arange(6,frames_num,8),:] = 0\n",
    "    noisy_frames[np.arange(7,frames_num,8),:] = 0\n",
    "\n",
    "    broken = noisy_frames.flatten()\n",
    "    \n",
    "    return clean, reconstructed, broken  \n",
    "\n",
    "def PESQ_quality_check(original_signal:np.ndarray,distorted_signal:np.ndarray,fs:int,frame_len:int,) -> float:\n",
    "    ### both the original and the reconstructed must be of the size n*frame_len where n is a natural number\n",
    "    original_signal_interpolated = resample(original_signal, int(original_signal.size*2))\n",
    "    distorted_signal_interpolated = resample(distorted_signal, int(distorted_signal.size*2))\n",
    "    fs = fs*2\n",
    "    N = int(original_signal_interpolated.size//frame_len)\n",
    "    original_tensor = torch.from_numpy(original_signal_interpolated)\n",
    "    distorted_tensor = torch.from_numpy(distorted_signal_interpolated)\n",
    "    pesq_loss = perceptual_evaluation_speech_quality(original_tensor,distorted_tensor,fs,\"wb\")\n",
    "\n",
    "    return pesq_loss.item()\n",
    "\n",
    "def SegSNR_quality_check(original_signal:np.ndarray,distorted_signal:np.ndarray,fs:int,frame_len:int,) -> float:\n",
    "    ### both the original and the reconstructed must be of the size n*frame_len where n is a natural number\n",
    "    N = int(original_signal.size//frame_len)\n",
    "    noise_signal = distorted_signal-original_signal\n",
    "    original_tensor = torch.from_numpy(original_signal)\n",
    "    distorted_tensor = torch.from_numpy(distorted_signal)\n",
    "  \n",
    "    pesq_loss = perceptual_evaluation_speech_quality(original_tensor,distorted_tensor,fs,\"nb\")\n",
    "    \n",
    "    SNRseg = np.zeros(original_signal.size)\n",
    "    original_signal_reshaped = original_signal.reshape((N,frame_len))\n",
    "    noise_signal_reshaped = noise_signal.reshape((N,frame_len))\n",
    "    for i in range(N):\n",
    "        P_original = sum(original_signal_reshaped[i,:] ** 2)\n",
    "        P_noise = sum(noise_signal_reshaped[i,:] ** 2)\n",
    "        SNRseg[i] = 10*np.log10(P_original/P_noise)\n",
    "    segmental_snr_value = sum(SNRseg)/N\n",
    "\n",
    "    return segmental_snr_value        \n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to your ID numbers, load the chosen speech file.  \n",
    "Pad the signal with zeros so it will contain an integer number of frames.  \n",
    "Re-define the 'voiced' and 'unvoiced' frames, the same way you did in the last HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_digit = 1 # insert your ID digit\n",
    "\n",
    "# load speech signal\n",
    "speechfilename = 'speech'+str(id_digit)+'.wav'\n",
    "fs, speech_signal = ...\n",
    "\n",
    "# Pad the signal with zeros\n",
    "frameLen = 256 \n",
    "speech_signal_padded = ...\n",
    "\n",
    "# re-define the voiced and unvoiced exaple frames frm last meeting\n",
    "voiced_frame_index = int(...)\n",
    "voiced_example_frame = speech_signal_padded[(voiced_frame_index*frameLen):((voiced_frame_index+1)*frameLen)]\n",
    "\n",
    "unvoiced_frame_index = int(...)\n",
    "unvoiced_example_frame = speech_signal_padded[(unvoiced_frame_index*frameLen):((unvoiced_frame_index+1)*frameLen)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop that will apply 'vu_classify' function, frame-by-frame, on the whole signal.  \n",
    "plot the signal and the pitch values using the 'plot_pitch' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your implementation of the function 'residual_energy' (from the previous parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_energy(signal: np.ndarray, FIR_coeffs: np.ndarray) -> int:\n",
    "    # insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the lpc coefficients using 'lpc' function.  \n",
    "Calculate the residual error using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lpc(...)\n",
    "e = residual_energy(...)\n",
    "print(f'The prediction filter coefficients are:')\n",
    "print(a)\n",
    "print(f'The prediction error is: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the spectrum of the frame, along with the spectrum of the lpc filter using the function 'plot_frame_and_filter_spectrum'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvec = ...\n",
    "\n",
    "_,H = freqz([1], a, frameLen//2)\n",
    "H_dB = ...\n",
    "\n",
    "voiced_example_frame_F_dB = ...\n",
    "\n",
    "plot_frame_and_filter_spectrum(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat last section with other 'p' values, according to the booklet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code for the encoder function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(signal: np.ndarray) -> (int,np.ndarray,np.ndarray,np.ndarray):\n",
    "    frame_len = 256\n",
    "    sampling_freq = 8000    \n",
    "    LPCorder = ...\n",
    "    \n",
    "    # Pad signal with zeros to contain an integer number of frames\n",
    "    reminder_samp = signal.size%frame_len\n",
    "    if(reminder_samp > 0):\n",
    "        pad_num = (frame_len-reminder_samp) # number of zeros for padding\n",
    "        signal_padded = np.pad(array=signal, pad_width=(0, pad_num))\n",
    "    else:\n",
    "        signal_padded = signal           \n",
    "    \n",
    "    # Divide the signals to frames\n",
    "    speech_framed = np.reshape(a=signal_padded, newshape=(signal_padded.size//frame_len, frame_len)) #Each 256 slice is a row    \n",
    "    \n",
    "    # Encoder's variables init:\n",
    "    ##### complete the dimensions ###\n",
    "    N = ...                   # N is frames_num\n",
    "    vu_flag = np.zeros(...)   # representing all frames \n",
    "    p = np.zeros(...)         # pitch duration for all frames. unvoiced frames are set to 0 pitch values. voiced frames are represented by the *index value* of the pitch (not in Hz)\n",
    "    lp = np.zeros((..., ...)) # estimated LPC filter's coeffs for all frames\n",
    "    e = np.zeros(...)         # the square root of the residual energies for all frames\n",
    "    \n",
    "    # Encoding process\n",
    "    for i in range(N): # for each frame\n",
    "        vu_flag[i] = ...\n",
    "        if( ... ):\n",
    "            p[i] = ...\n",
    "        \n",
    "        lp[:,i] = lpc(...,...) # estimate the transfer function of the mouth\n",
    "        e[i] = np.sqrt(...) # average energy using residual_energy function\n",
    "\n",
    "    return (N,p,lp,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the encoder function on the original signal (not the padded version) to see that it is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,p,lp,e = encoder(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 18.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your implementation of the function 'generate_impulse_train_frame' (from the perliminary report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_impulse_train_frame(frame_len: int, pitch_period: int, reminder_from_last_frame: int) -> (np.ndarray, int):\n",
    "    # insert code here\n",
    "    return frame, reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code for the decoder function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(N: int, p: np.ndarray ,lp: np.ndarray, e: np.ndarray) -> (np.ndarray):\n",
    "    frame_len = 256\n",
    "    estimated_frames = np.zeros((..., frame_len))  # representation of the synthesized frames of the decoded signal as a matrix (according to the number of frames)\n",
    "    rem_last_frame = 0 # init of the the reminder-index (the gap from the end of the last frame to the location of the last impulse in the last frame)\n",
    "    final_cond = np.zeros(...) # according to question 4 in the preliminary questions section in the booklet\n",
    "\n",
    "    # Decoding process\n",
    "    for i in range(N):   # for each frame\n",
    "        # create exitation signal according to u/v decision\n",
    "        if( ... ): # unvoiced    \n",
    "            excitation_frame = np.random.randn(frame_len) # white noise\n",
    "            rem_last_frame = ...\n",
    "        else: # voiced\n",
    "            excitation_frame, rem_next_frame = generate_impulse_train_frame(...) # impulse train\n",
    "            rem_last_frame = ...\n",
    "\n",
    "        # normalize the excitation signal and amplify it with the gain value. \n",
    "        # *DO NOT* forget to use square root of 'normalization_energy' in order to normalize the excitation signal\n",
    "        normalization_energy = np.sum(excitation_frame**2)/frame_len\n",
    "        excitation_frame_normalized = ... \n",
    "        excitation_frame_amplified = excitation_frame_normalized * ...\n",
    "        \n",
    "        # filter the exitation signal with the LPC coefficients, this time use it as an 'all-pole' filter\n",
    "        estimated_frames[i,:], final_cond = lfilter(..., ..., ..., zi=final_cond)\n",
    "\n",
    "    # concatenate all frames to a 1-dimentional signal\n",
    "    return estimated_frames.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the decoder to get a synthesized version of the signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_signal = decoder(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the original and the synthesized signal (you can remove the padded section from 'estimated_signal' so both vectors will have the same size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the two audio files and compare how they sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save sythesized signal to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinationfilename = 'speech'+str(id_digit)+'_synthesized.wav'\n",
    "write_wav_file(destinationfilename, fs, estimated_signal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "99f5f5da5e50376999c86646ae614d55375ac95cb655839ebbb2eb852a00cc73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
