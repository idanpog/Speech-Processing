{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note:  \n",
    "In some places you will see the line \"if __name__ == \"__main__\":\" in the code.  \n",
    "It enables to conviniently use the code generated here in the notebook of the next PART.  \n",
    "**Feel Free to ignore it, but don't delete it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions and Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from scipy.signal import correlate, find_peaks, lfilter\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from math import pi\n",
    "# For displaying interactive plots in the notebook\n",
    "%matplotlib widget\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "def read_wav_file(filename: str) -> (int, np.ndarray):\n",
    "    Fs, signal = read(filename)\n",
    "    signal = signal.astype(dtype=np.float32)\n",
    "    signal = signal/(2**15)\n",
    "    return Fs, signal\n",
    "\n",
    "def basic_plot(x, y, xlabel='', ylabel='', title='', grid=True) -> None:\n",
    "    fig = plt.figure()\n",
    "    line, = plt.plot(x, y)\n",
    "    y_range = np.max(y)-np.min(y)\n",
    "    plt.axis([x[0], x[x.size-1], np.min(y)-0.1*y_range, np.max(y)+0.1*y_range])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if(grid):\n",
    "        plt.grid()\n",
    "    plt.title(title)\n",
    "    plt.locator_params(axis='x',tight='true')\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to your ID numbers, load the chosen speech file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of recording is 16578 samples\n",
      "Sampling frequancy: 8000 Hz\n"
     ]
    }
   ],
   "source": [
    "id_digit = 5 # insert your ID digit\n",
    "\n",
    "# load speech signal\n",
    "speechfilename = '../SpeechSignalFiles/speech'+str(id_digit)+'.wav'\n",
    "fs, speech_signal = read_wav_file(speechfilename)\n",
    "print(f'Length of recording is {speech_signal.size} samples')\n",
    "print(f'Sampling frequancy: {fs} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the signal with zeros according to the instructions in the experiment's booklet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of zeros padded 62\n",
      "Current length of recording is 16640 samples\n"
     ]
    }
   ],
   "source": [
    "# Pad the signal with zeros\n",
    "from math import ceil\n",
    "\n",
    "frameLen = 256\n",
    "pad_num = ceil(speech_signal.size/frameLen)*frameLen - speech_signal.size\n",
    "print(f'Amount of zeros padded {pad_num}')\n",
    "speech_signal_pad = np.pad(array=speech_signal, pad_width=(0, pad_num))\n",
    "print(f'Current length of recording is {speech_signal_pad.size} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the file played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play file\n",
    "Audio(data=speech_signal_pad, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the function 'create_time_vector' from your 'PART1.ipynb' file to here.  \n",
    "Plot the padded speech signal using 'create_time_vector' and 'basic_plot' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_vector(vector_length: int, sampling_freq: int, start_time: float=0) -> np.ndarray:\n",
    "    # insert code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the padded speech signal using 'create_time_vector' and 'basic_plot' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    tvec = ...\n",
    "    basic_plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the different sections in the signal in order to identify the start and end points of each word.  \n",
    "Write the start and end time for each word in the following code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play different sections in the speech file\n",
    "start_time = ...   #sec\n",
    "end_time = ...     # sec\n",
    "\n",
    "Audio(data=speech_signal_pad[int(np.round(start_time*fs)):int(np.round(end_time*fs))], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Fill in the missing times and word strings (for words that doesn't exist use time zero and an empty string)\n",
    "    \n",
    "    # Word No. 1\n",
    "    word1_start_time = ...   # sec\n",
    "    word1_end_time = ...     # sec\n",
    "    Word1_string = '...'\n",
    "    # Word No. 2\n",
    "    word2_start_time = ...   # sec\n",
    "    word2_end_time = ...     # sec\n",
    "    Word2_string = '...'\n",
    "    # Word No. 3\n",
    "    word3_start_time = ...   # sec\n",
    "    word3_end_time = ...     # sec\n",
    "    Word3_string = '...'\n",
    "    # Word No. 4\n",
    "    word4_start_time = ...   # sec\n",
    "    word4_end_time = ...     # sec\n",
    "    Word4_string = '...'\n",
    "    # Word No. 5\n",
    "    word5_start_time = ...   # sec\n",
    "    word5_end_time = ...     # sec\n",
    "    Word5_string = '...'\n",
    "    # Word No. 6\n",
    "    word6_start_time = ...   # sec\n",
    "    word6_end_time = ...     # sec\n",
    "    Word6_string = '...'\n",
    "    # Word No. 7\n",
    "    word7_start_time = ...   # sec\n",
    "    word7_end_time = ...     # sec\n",
    "    Word7_string = '...'\n",
    "    # Word No. 8\n",
    "    word8_start_time = ...   # sec\n",
    "    word8_end_time = ...     # sec\n",
    "    Word8_string = '...'\n",
    "    # Word No. 9\n",
    "    word9_start_time = ...   # sec\n",
    "    word9_end_time = ...     # sec\n",
    "    Word9_string = '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your code from 3 cells ago in the places with <...> and run the following code section  \n",
    "to plot the signal again with the different words marked on the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "def mark_area_with_color(from_time, to_time, x, y, color_string) -> None:\n",
    "    plt.axhspan(np.min(y), np.max(y), from_time/x[x.size-1], to_time/x[x.size-1], color=color_string, alpha=0.5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # plot signal\n",
    "    basic_plot(...)    \n",
    "    # mark words on signal\n",
    "    speech_signal_pad_y_range = np.max(speech_signal_pad)-np.min(speech_signal_pad)\n",
    "    mark_area_with_color(word1_start_time, word1_end_time, tvec, speech_signal_pad, 'yellow')\n",
    "    plt.text(x=word1_start_time, y=np.max(speech_signal_pad)-0.1*speech_signal_pad_y_range, s=Word1_string);\n",
    "    mark_area_with_color(word2_start_time, word2_end_time, tvec, speech_signal_pad, 'red')\n",
    "    plt.text(x=word2_start_time, y=np.max(speech_signal_pad)-0.2*speech_signal_pad_y_range, s=Word2_string);\n",
    "    mark_area_with_color(word3_start_time, word3_end_time, tvec, speech_signal_pad, 'brown')\n",
    "    plt.text(x=word3_start_time, y=np.max(speech_signal_pad)-0.3*speech_signal_pad_y_range, s=Word3_string);\n",
    "    mark_area_with_color(word4_start_time, word4_end_time, tvec, speech_signal_pad, 'orange')\n",
    "    plt.text(x=word4_start_time, y=np.max(speech_signal_pad)-0.4*speech_signal_pad_y_range, s=Word4_string);\n",
    "    mark_area_with_color(word5_start_time, word5_end_time, tvec, speech_signal_pad, 'green')\n",
    "    plt.text(x=word5_start_time, y=np.max(speech_signal_pad)-0.5*speech_signal_pad_y_range, s=Word5_string);\n",
    "    mark_area_with_color(word6_start_time, word6_end_time, tvec, speech_signal_pad, 'purple')\n",
    "    plt.text(x=word6_start_time, y=np.max(speech_signal_pad)-0.6*speech_signal_pad_y_range, s=Word6_string);\n",
    "    mark_area_with_color(word7_start_time, word7_end_time, tvec, speech_signal_pad, 'blue')\n",
    "    plt.text(x=word7_start_time, y=np.max(speech_signal_pad)-0.7*speech_signal_pad_y_range, s=Word7_string);\n",
    "    mark_area_with_color(word8_start_time, word8_end_time, tvec, speech_signal_pad, 'cyan')\n",
    "    plt.text(x=word8_start_time, y=np.max(speech_signal_pad)-0.8*speech_signal_pad_y_range, s=Word8_string);\n",
    "    mark_area_with_color(word9_start_time, word9_end_time, tvec, speech_signal_pad, 'black')\n",
    "    plt.text(x=word9_start_time, y=np.max(speech_signal_pad)-0.9*speech_signal_pad_y_range, s=Word9_string);\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot again the padded speech signal using 'create_time_vector' and 'basic_plot' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 'zoom' option to inspect the signal from close view:  \n",
    "**Zoom in** using the 'square' icon, **Zoom out** using the 'house' icon.  \n",
    "find two segments that you can say that are clearly \"voiced\", and two segments that are clearly \"unvoiced\".  \n",
    "Then, fill in the code in the following code segment to receive a graph with the chosen areas marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "def mark_as_voiced(from_time, to_time, x, y) -> None:\n",
    "    # mark a voiced area in red\n",
    "    mark_area_with_color(from_time, to_time, x, y, 'red')\n",
    "    plt.text(x=from_time, y=np.max(y)-0.1*(np.max(y)-np.min(y)), s='Voiced')\n",
    "    \n",
    "def mark_as_unvoiced(from_time, to_time, x, y) -> None:\n",
    "    # mark a unvoiced area in orange\n",
    "    mark_area_with_color(from_time, to_time, x, y, 'orange')\n",
    "    plt.text(x=from_time, y=np.max(y)-0.1*(np.max(y)-np.min(y)), s='Unvoiced')    \n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    voiced1_start_time = ...     # sec\n",
    "    voiced1_end_time = ...       # sec\n",
    "    \n",
    "    voiced2_start_time = ...     # sec\n",
    "    voiced2_end_time = ...       # sec\n",
    "    \n",
    "    unvoiced1_start_time = ...   # sec\n",
    "    unvoiced1_end_time = ...     # sec\n",
    "    \n",
    "    unvoiced2_start_time = ...   # sec\n",
    "    unvoiced2_end_time = ...     # sec\n",
    "    \n",
    "    # plot signal\n",
    "    basic_plot(...)      \n",
    "    # mark segments on signal    \n",
    "    mark_as_voiced(voiced1_start_time, voiced1_end_time, tvec, speech_signal_pad)\n",
    "    mark_as_voiced(voiced2_start_time, voiced2_end_time, tvec, speech_signal_pad)\n",
    "    mark_as_unvoiced(unvoiced1_start_time,unvoiced1_end_time, tvec, speech_signal_pad)\n",
    "    mark_as_unvoiced(unvoiced2_start_time, unvoiced2_end_time, tvec, speech_signal_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the padded signal into frames of 256 samples each.\n",
    "Pick one frame as \"voiced\" example frame and one frame as \"unvoiced\" example frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    frameLen = 256\n",
    "    speech_signal_as_martix = np.reshape(a=... , newshape=(..., ...)) #Each 256 slice is a row\n",
    "    \n",
    "    # choose voiced frame\n",
    "    voiced_frame_index = ...\n",
    "    print(f'Voiced example frame index is:  {voiced_frame_index}')\n",
    "    voiced_example_frame = speech_signal_as_martix[voiced_frame_index]\n",
    "    basic_plot(...)\n",
    "    \n",
    "    # choose unvoiced frame\n",
    "    unvoiced_frame_index = ...\n",
    "    print(f'Unvoiced example frame index is:  {unvoiced_frame_index}')\n",
    "    unvoiced_example_frame = speech_signal_as_martix[unvoiced_frame_index]\n",
    "    basic_plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the voiced example frame with x-axis defined to be samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate pitch values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pitch_period_in_samples = ...\n",
    "    pitch_period_in_seconds = ...\n",
    "    pitch_period_freq = ..\n",
    "    \n",
    "    print(f'The length of the pitch period is {pitch_period_in_samples} samples')\n",
    "    print(f'The length of the pitch period is {pitch_period_in_seconds} seconds')\n",
    "    print(f'The pitch frequency is {pitch_period_freq:.2f} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the functions 'create_freq_vector' from the prep1 file to here.  \n",
    "Plot the spectrum of the voiced frame using 'create_freq_vector' and 'basic_plot' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freq_vector(signal_length: int, sampling_freq: int, only_positive_half: bool=True) -> np.ndarray:\n",
    "    # insert code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # pad the frame with zeros before using fft on it (ONLY IN THIS SECTION)\n",
    "    voiced_example_frame_padded = ...\n",
    "    voiced_example_frame_padded_F = ...\n",
    "    voiced_example_frame_padded_F_dB = ...\n",
    "    fvec = ...\n",
    "    basic_plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark on the graph two of the peaks that you recognize as pitch multiples, and run the code (they will show on the graph above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # mark two pitch \"peaks\" you recognize\n",
    "    pitch_pick_freq1 = ... # Hz\n",
    "    pitch_pick_freq2 = ... # Hz\n",
    "\n",
    "    line = plt.plot([pitch_pick_freq1, pitch_pick_freq1], [np.min(voiced_example_frame_padded_F_dB), np.max(voiced_example_frame_padded_F_dB)], '-', color=\"orange\")\n",
    "    line = plt.plot([pitch_pick_freq2, pitch_pick_freq2], [np.min(voiced_example_frame_padded_F_dB), np.max(voiced_example_frame_padded_F_dB)], '-', color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - Mark the envelope curve also on the spectrum graph.  \n",
    "(or, do it manually on a snapshot image using some drawing tool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create a rough sketch of the envelope curve\n",
    "    curve_x_values = np.array([, , , , , , , , ])\n",
    "    curve_y_values = np.array([, , , , , , , , ])\n",
    "    \n",
    "    line = plt.plot(curve_x_values, curve_y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the auto-correlation curve of the ORIGINAL voiced frame (not the padded one).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ac = ...\n",
    "    lags = ...\n",
    "    basic_plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the graph above to measure the length of the pitch period in samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pitch_period_from_ac_in_samples = ... # samples\n",
    "    pitch_period_from_ac_in_seconds = ... # seconds\n",
    "    pitch_period_from_ac_freq = ...       # Hz\n",
    "    \n",
    "    print(f'The length of the pitch period is {pitch_period_from_ac_in_samples} samples')\n",
    "    print(f'The length of the pitch period is {pitch_period_from_ac_in_seconds} seconds')\n",
    "    print(f'The pitch frequency is {pitch_period_from_ac_freq:.2f} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your implementation to \"pitch_detect_corr\", as was in PART 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_detect_corr(signal: np.ndarray, sampling_freq: int) -> int:\n",
    "    # insert code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the \"pitch_detect_corr\" function on the voiced example frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pitch_duration_from_func = pitch_detect_corr(...)\n",
    "    pitch_period_from_func_seconds = ...\n",
    "    pitch_period_from_func_freq = ...\n",
    "    \n",
    "    print(f'The length of the pitch period is {pitch_duration_from_func} samples')\n",
    "    print(f'The length of the pitch period is {pitch_period_from_func_seconds} seconds')\n",
    "    print(f'The pitch frequency is {pitch_period_from_func_freq} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the spectrum of the un-voiced frame. No need to pad the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    unvoiced_example_frame_F = ...\n",
    "    unvoiced_example_frame_F_dB = ...\n",
    "    fvec = ...\n",
    "    basic_plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the auto-correlation curve of the unvoiced frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    y_ac = ...\n",
    "    x_ac = ...\n",
    "    basic_plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the \"zero_cross\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_cross(frame: np.ndarray) -> int:\n",
    "    # insert code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to verify \"zero_cross\" implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sig1 = np.sin(2*pi*(np.arange(0, 256)/80))\n",
    "    sig2 = np.random.randn(256)\n",
    "    basic_plot(...)\n",
    "    basic_plot(...)\n",
    "    print(f'The number of zero crossing in sig1 is {zero_cross(sig1)}')\n",
    "    print(f'The number of zero crossing in sig2 is {zero_cross(sig2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the \"short_time_energy\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_time_energy(frame: np.ndarray) -> float:\n",
    "    # insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing code for the \"vu_classify\" function, according to the instructions in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vu_classify(frame: np.ndarray, sampling_freq: int) -> bool:\n",
    "    N = len(frame)\n",
    "    assert N == 256\n",
    "    avg_energy = ...\n",
    "    if(avg_energy < ...):\n",
    "        res = False\n",
    "    else:\n",
    "        # Condition 1 - Check zero crossing\n",
    "        cond1 = ...\n",
    "        # Condition 2 - Check energy\n",
    "        cond2 = ...\n",
    "        # Condition 3 - Check whether highest local peak is out of pitch range     \n",
    "        pitch_f_max = ... # [Hz]  according to pitch frequencies range\n",
    "        pitch_f_min = ... # [Hz]  according to pitch frequencies range\n",
    "        pitch_range_min_index = sampling_freq/pitch_f_max\n",
    "        pitch_range_max_index = sampling_freq/pitch_f_min\n",
    "\n",
    "        frame_ac = correlate(frame, frame)[(frame.size-1):] # get lags from zero to end\n",
    "        peaks_indexes, _ = find_peaks(frame_ac)\n",
    "        max_peak_index = peaks_indexes[np.argmax(frame_ac[peaks_indexes])]\n",
    "        cond3 =  max_peak_index > pitch_range_max_index or max_peak_index < pitch_range_min_index\n",
    "        # Condition 4 - Check if main peak is meaningful\n",
    "        cond4 = ...\n",
    "        # Check voiced/unvoiced\n",
    "        res = (int(cond1)+int(cond2)+int(cond3)+int(cond4)) < 2\n",
    "    \n",
    "    return bool(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation of \"vu_classify\", using sig1 and sig2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print('uv_classify results:')\n",
    "    print(f'For voiced-like frame: {vu_classify(sig1, fs}')\n",
    "    print(f'For unvoiced-like frame: {vu_classify(sig2, fs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average energy of the voiced frame and the unvoiced frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(f'Average energy of voiced: {short_time_energy(voiced_example_frame):.7f}')\n",
    "    print(f'Average energy of unvoiced: {short_time_energy(unvoiced_example_frame):.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of zero crossing of the voiced frame and the unvoiced frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f'Zero crossing of voiced: {zero_cross(voiced_example_frame)}')\n",
    "    print(f'Zero crossing of unvoiced: {zero_cross(unvoiced_example_frame)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the classification of the voiced frame and the unvoiced frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f'Classification result of voiced: {vu_classify(voiced_example_frame, fs)}')\n",
    "    print(f'Classification result of unvoiced: {vu_classify(unvoiced_example_frame, fs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "99f5f5da5e50376999c86646ae614d55375ac95cb655839ebbb2eb852a00cc73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
